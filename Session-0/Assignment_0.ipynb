{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment-0.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MB743XFbgD4C",
        "colab_type": "text"
      },
      "source": [
        "# Assignment-0\n",
        "## 0.1 Understanding a Basic Neural Network\n",
        "In Session-0, we wrote this code for generating a neural network with a single hidden layer. We performed forward and backward propagation and minimized the loss of the XOR gate's truth table. \n",
        "\n",
        "For the assignment, you should write a document that consists of your understanding and analysis of the following barebones NeuralNetwork Class. There are 3 tasks you should complete.\n",
        "\n",
        "**Note:** Matplotlib is a library used to plot graphs. You can ignore the parts of the code where matplotlib is used in this assignment notebook. The generated graphs are for your understanding. However, you will have to capture screenshots the resulting graphs from each task (1,2,3) and write about them as a part of the assignment. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "coK2vQSrRqZ5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V6GYUOgMR0BR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NeuralNetwork():\n",
        "  def __init__(self, input_size, hidden_size, output_size):\n",
        "    super(NeuralNetwork, self).__init__()\n",
        "    self.weights_input_to_hidden = np.random.random((input_size, hidden_size))\n",
        "    self.weights_hidden_to_output = np.random.random((hidden_size, output_size))\n",
        "\n",
        "  # For Task 2, change the sigmoid function to tan-h and reLU here\n",
        "  ## TASK 2 CODE STARTS HERE\n",
        "  def sigmoid(self, x, deriv = False):\n",
        "    if deriv:\n",
        "      return x * (1 - x)\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "  ## TASK 2 CODE ENDS HERE\n",
        "\n",
        "  def train(self, train_x, train_y, num_epochs):\n",
        "    loss_dict = {}\n",
        "    for epoch in range(num_epochs):\n",
        "        # Forward prop\n",
        "        self.l0 = train_x\n",
        "        self.l1 = self.sigmoid(np.dot(self.l0, self.weights_input_to_hidden))\n",
        "        l2 = self.sigmoid(np.dot(self.l1, self.weights_hidden_to_output))\n",
        "\n",
        "        # Backprop\n",
        "        # Finding final and hidden layer losses\n",
        "        loss = train_y - l2\n",
        "        if epoch % 1000 == 0:\n",
        "          print('Epoch {}/{} \\tLoss:{}'.format(epoch+1, num_epochs, np.mean(np.abs(loss))))\n",
        "          #plt.plot(epoch+1,np.mean(loss))\n",
        "        \n",
        "        l2_delta = loss * self.sigmoid(l2, deriv = True)\n",
        "        l1_error = l2_delta.dot(self.weights_hidden_to_output.T)\n",
        "        l1_delta = l1_error * self.sigmoid(self.l1, deriv = True)\n",
        "        \n",
        "        # Optimizing weights\n",
        "        self.weights_hidden_to_output += self.l1.T.dot(l2_delta)\n",
        "        self.weights_input_to_hidden += self.l0.T.dot(l1_delta)\n",
        "\n",
        "        # Store loss in a dictionary\n",
        "        loss_dict[epoch] = np.abs(np.mean(loss))\n",
        "    return loss_dict\n",
        "        \n",
        "  def test(self, test_x):\n",
        "    self.l0 = test_x\n",
        "    self.l1 = self.sigmoid(np.dot(self.l0, self.weights_input_to_hidden))\n",
        "    output = self.sigmoid(np.dot(self.l1, self.weights_hidden_to_output))\n",
        "    if output < 0.5:\n",
        "      return 0\n",
        "    return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZOmo20zR-PL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Training Set\n",
        "arr_x = np.array([[0,0,0],\n",
        "                [1,1,1],\n",
        "                [1,0,0],\n",
        "                [0,0,1],\n",
        "                [1,1,0],\n",
        "                [1,0,1]])\n",
        "arr_y = np.array([[0],\n",
        "                 [1],\n",
        "                 [1],\n",
        "                 [1],\n",
        "                 [0],\n",
        "                 [0]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qHmvLZystkCK",
        "colab_type": "text"
      },
      "source": [
        "### Task 1\n",
        "The same `XOR_using_NN` code is copied here. You need to observe the loss in the following conditions:\n",
        "\n",
        "1. Change the size of hidden layer, *hidden_size*.\n",
        "2. Change the total number of epochs the model will run for, *num_epochs*. Remember, a single epoch is just one iteration of the training set in forward prop and backprop.\n",
        "\n",
        "Change the *hidden_size* in the class initialization line. Change the *num_epochs* in the training data line. \n",
        "\n",
        "\n",
        "**Hint:** Reduce the *hidden_size* to 1, reduce the *num_epochs* to 50. Try to find the optimal number for *hidden_size* at which the loss starts to reduce and do the same thing for *num_epochs*.\n",
        "\n",
        "Observe the resulting loss and write your inference in a document. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qCG3LHLiSB2j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nn = NeuralNetwork(input_size=3, hidden_size = 5, output_size = 1)\n",
        "loss = nn.train(train_x = arr_x, train_y = arr_y, num_epochs = 5000)\n",
        "plt.plot(list(loss.keys()),list(loss.values()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "owndItCAv3ky",
        "colab_type": "text"
      },
      "source": [
        "### Task 2\n",
        "Next, instead of using Sigmoid as the activation function, use Tan-H and ReLU. \n",
        "\n",
        "1. Tan-H is inbuilt in numpy, use `np.tanh` to define the function.\n",
        "2. ReLU is 0 for x < 0, x for x > 0. Define a function using these constraints. \n",
        "\n",
        "These function are defined in `0.2 Activation Functions`. Replace `sigmoid` with your function's name in each part of the class defined (scroll up) and run the same training data (6 permutations of the XOR truth table). Observe the loss and write your inference in the same document."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KFWy1lToxWdG",
        "colab_type": "text"
      },
      "source": [
        "### Task 3\n",
        "Finally, let's observe results using a different truth table. Generate the truth tables for the following logical expressions:\n",
        "\n",
        "1. F = !((A.B)+C) + D\n",
        "2. F = !(A.B) xor !(C.D)\n",
        "\n",
        "Use `np.array` to define the resulting truth table. Remember, the inputs (A,B,C,D) are store in variable `x` and the output (F) is stored in variable `y`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cf04xb3CxaFg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Training Set for Task 3\n",
        "## TASK 3 CODE STARTS HERE\n",
        "x =\n",
        "y ="
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rDgfVQPGzmGt",
        "colab_type": "text"
      },
      "source": [
        "Then, initialize the class and run the training set. Remember, our input size has changed to 4. So, change the *input_size* argument in the class initialization accordingly. \n",
        "\n",
        "Once again, observe the loss by playing around with *hidden_size* and *num_epochs*. Write about your results in the document. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2pea4lozyc7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nn_task3 = NeuralNetwork(input_size=3, hidden_size = 5, output_size = 1)\n",
        "loss = nn_task3.train(train_x = arr_x, train_y = arr_y, num_epochs = 5000)\n",
        "plt.plot(list(loss.keys()),list(loss.values()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41qZRurhdqUP",
        "colab_type": "text"
      },
      "source": [
        "## 0.2 Activation Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5vmrilnedwpv",
        "colab_type": "text"
      },
      "source": [
        "**This section is not a part of the assignment. Use this section to visualize the activation functions.** \n",
        "\n",
        "There are several activation functions available. In session-0, we discussed about Sigmoid, Tan-h and Rectified Linear Unit (ReLU). The functions are presented below. Run each cell and observe the graphs. Toggle the *deriv* argument, change the range, and observe the results. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2bqkEX_ALr7y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Sigmoid Function\n",
        "def sigmoid(x, deriv = False):\n",
        "  if deriv:\n",
        "    return x * (1 - x)\n",
        "  return 1 / (1 + np.exp(-x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DIdd2ldwJK7b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Sigmoid Function\n",
        "# toggle deriv = True/False and range; observe the graphs\n",
        "numbers_sigmoid = [sigmoid(num,deriv=True) for num in range(-10,10)]\n",
        "plt.plot(numbers_sigmoid)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rEgWMsREMNEo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TanH Function\n",
        "def tanh(x, deriv = False):\n",
        "  f = np.tanh(x)\n",
        "  if deriv:\n",
        "    return 1 - f**2\n",
        "  return f"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "05VdW95dM7OW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TanH Function\n",
        "# toggle deriv = True/False and range; observe the graphs\n",
        "numbers_tanh = [tanh(num,deriv=False) for num in range(-10,10)] \n",
        "plt.plot(numbers_tanh)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NoySVCFvedtn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Rectified Linear Unit (ReLU) Function\n",
        "def relu(x, deriv = False):\n",
        "  if deriv:\n",
        "    if x > 0:\n",
        "      return 1\n",
        "    else:\n",
        "      return 0\n",
        "  else:\n",
        "    return max(0,x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mQO4ZaH4fBw-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ReLU Function\n",
        "# toggle deriv = True/False and range; observe the graphs\n",
        "numbers_relu = [relu(num,deriv=False) for num in range(-10,10)]\n",
        "plt.plot(numbers_relu)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}