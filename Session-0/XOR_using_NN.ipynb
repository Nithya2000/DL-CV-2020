{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "coK2vQSrRqZ5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V6GYUOgMR0BR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NeuralNetwork():\n",
        "  def __init__(self, input_size, hidden_size, output_size):\n",
        "    super(NeuralNetwork, self).__init__()\n",
        "    self.weights_input_to_hidden = np.random.random((input_size, hidden_size))\n",
        "    self.weights_hidden_to_output = np.random.random((hidden_size, output_size))\n",
        "\n",
        "  def sigmoid(self, x, deriv = False):\n",
        "    if deriv:\n",
        "      return x * (1 - x)\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "  def train(self, train_x, train_y, num_epochs):\n",
        "    for epoch in range(num_epochs):\n",
        "        # FORWARD PROP\n",
        "        self.l0 = train_x\n",
        "        self.l1 = self.sigmoid(np.dot(self.l0, self.weights_input_to_hidden))\n",
        "        l2 = self.sigmoid(np.dot(self.l1, self.weights_hidden_to_output))\n",
        "\n",
        "        # BACKPROP STARTS HERE\n",
        "        # Finding final and hidden layer losses\n",
        "        loss = train_y - l2\n",
        "        if epoch % 1000 == 0:\n",
        "          print('Epoch {}/{} \\tLoss:{}'.format(epoch+1, num_epochs, np.mean(np.abs(loss))))\n",
        "        \n",
        "        l2_delta = loss * self.sigmoid(l2, deriv = True)\n",
        "        l1_error = l2_delta.dot(self.weights_hidden_to_output.T)\n",
        "        l1_delta = l1_error * self.sigmoid(self.l1, deriv = True)\n",
        "        \n",
        "        # Optimizing weights\n",
        "        self.weights_hidden_to_output += self.l1.T.dot(l2_delta)\n",
        "        self.weights_input_to_hidden += self.l0.T.dot(l1_delta)\n",
        "\n",
        "  def test(self, test_x):\n",
        "    self.l0 = test_x\n",
        "    self.l1 = self.sigmoid(np.dot(self.l0, self.weights_input_to_hidden))\n",
        "    output = self.sigmoid(np.dot(self.l1, self.weights_hidden_to_output))\n",
        "    if output < 0.5:\n",
        "      return 0\n",
        "    return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZOmo20zR-PL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TRAIN DATA\n",
        "arr_x = np.array([[0,0,0],\n",
        "                [1,1,1],\n",
        "                [1,0,0],\n",
        "                [0,0,1],\n",
        "                [1,1,0],\n",
        "                [1,0,1]])\n",
        "arr_y = np.array([[0],\n",
        "                 [1],\n",
        "                 [1],\n",
        "                 [1],\n",
        "                 [0],\n",
        "                 [0]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qCG3LHLiSB2j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nn = NeuralNetwork(input_size=3, hidden_size = 5, output_size = 1)\n",
        "nn.train(train_x = arr_x, train_y = arr_y, num_epochs = 10000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sEweBXsgSGpS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nn.test(np.array([0,1,0]))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}